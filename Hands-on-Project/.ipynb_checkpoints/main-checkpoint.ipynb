{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d7504c-883b-4b5a-b233-4982c2c8436b",
   "metadata": {},
   "source": [
    "### Class for Managing Spark and Hive Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50285afb-86bb-4976-8b80-0a9ae7ed985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pyspark.sql import SparkSession\n",
    "from os.path import abspath\n",
    "\n",
    "@dataclass\n",
    "class BigDataProcessing():\n",
    "\n",
    "    warehouse_location = abspath('spark-warehouse\\one')\n",
    "    \n",
    "    # Class variable\n",
    "    spark = SparkSession.builder \\\n",
    "                        .appName(\"DataProcess\") \\\n",
    "                        .enableHiveSupport() \\\n",
    "                        .getOrCreate()\n",
    "\n",
    "                            # .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "\n",
    "    @classmethod\n",
    "    def get_data_frame(cls, path):\n",
    "        \"\"\"\n",
    "            Upload the data to spark and return the Data Frame\n",
    "        \"\"\"\n",
    "        return cls.spark.read.csv(path = path, header = True, inferSchema = True)\n",
    "\n",
    "    @classmethod\n",
    "    def create_hive_tables(cls) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Create a new Hive database\n",
    "        cls.spark.sql(\"CREATE DATABASE IF NOT EXISTS sparks\")\n",
    "\n",
    "        # Use the database\n",
    "        cls.spark.sql(\"USE sparks\")\n",
    "\n",
    "        # # Create table\n",
    "        # cls.spark.sql(\"\"\"\n",
    "        #     CREATE TABLE IF NOT EXISTS sparks.transaction(cust_id INT,\n",
    "        #                                     cust_name STRING,\n",
    "        #                                     cust_address STRING,\n",
    "        #                                     Trans_amt STRING,\n",
    "        #                                     Trans_id INT,\n",
    "        #                                     Trans_date STRING)\"\"\")\n",
    "        \n",
    "        # Describe the table to check its schema\n",
    "        # cls.spark.sql(\"DESCRIBE transaction_master\").show()\n",
    "\n",
    "    @classmethod\n",
    "    def retrive_hive_tables(cls, table_name: str) -> None:\n",
    "        print(\"From Hive : \\n\")\n",
    "        cls.spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").show()\n",
    "        cls.spark.sql(f\"SELECT * FROM {table_name} WHERE cust_address > \\\"chennai\\\"\").show()\n",
    "\n",
    "    @classmethod\n",
    "    def show_hive_tables(cls, table_name: str) -> None:\n",
    "        cls.spark.sql(\"SHOW DATABASES\").show()\n",
    "        cls.spark.sql(\"SHOW TABLES\").show()\n",
    "        cls.spark.sql(f\"DESCRIBE {table_name}\").show()\n",
    "        # cls.spark.sql(f\"SELECT * FROM {table_name}\").show()\n",
    "\n",
    "    @classmethod\n",
    "    def drop_hive_tables(cls, table_name: str) -> None:\n",
    "        cls.spark.sql(f\"DROP TABLE {table_name}\")\n",
    "        \n",
    "    @classmethod\n",
    "    def stop_spark_session(cls) -> None:\n",
    "        \"\"\"\n",
    "            Stops the spark session\n",
    "        \"\"\"\n",
    "        if cls.spark is not None and cls.spark.sparkContext is not None:\n",
    "            cls.spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3970ee-17a6-488d-b5b3-e10c49699cdf",
   "metadata": {},
   "source": [
    "### Actual Main (or) Driver Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e6fcff-20b2-4ff8-99c4-92e51ab0961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 12:52:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/08/23 12:52:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/08/23 12:52:07 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "24/08/23 12:52:07 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/08/23 12:52:07 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/08/23 12:52:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/08/23 12:52:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   sparks|\n",
      "+---------+\n",
      "\n",
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|   sparks|transaction_master|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n",
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|     cust_id|      int|   NULL|\n",
      "|   cust_name|   string|   NULL|\n",
      "|cust_address|   string|   NULL|\n",
      "|   Trans_amt|   string|   NULL|\n",
      "|    Trans_id|      int|   NULL|\n",
      "|  Trans_date|   string|   NULL|\n",
      "+------------+---------+-------+\n",
      "\n",
      "+-------+---------+------------+---------+--------+----------+\n",
      "|cust_id|cust_name|cust_address|Trans_amt|Trans_id|Trans_date|\n",
      "+-------+---------+------------+---------+--------+----------+\n",
      "|      1|      abc|     chennai|       2$|     100|  12-12-23|\n",
      "|      1|      ghi|       tamil|       5$|     110|  12-12-25|\n",
      "+-------+---------+------------+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "def unzip_file(file: str, dest_path: str, password: str = None) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Unzips the specified .zip file into the given directory.\n",
    "\n",
    "    :param file: file we need to extract\n",
    "    :dest_path: destination path were extracted files will be stored\n",
    "    :password: password if the zip is password protected else it was None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the directory if it doesn't exist for storing the extracted files\n",
    "        os.makedirs(dest_path, exist_ok=True)\n",
    "    \n",
    "        # If the file was protected with password\n",
    "        if password is not None:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # Extract all contents into the specified directory\n",
    "                zip_ref.extractall(dest_path, pwd=password)\n",
    "        else:\n",
    "            with zipfile.ZipFile(file, 'r') as zip_ref: \n",
    "                # Extract all contents into the specified directory\n",
    "                zip_ref.extractall(dest_path)\n",
    "    except:\n",
    "        print(\"Exception occurred in unzip_file\")\n",
    "\n",
    "def file_check(file_path: str, file: str) -> int:\n",
    "    \"\"\"\n",
    "        Get the file and return its size\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return os.path.getsize(file_path+'/'+file)\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "# Execution Starts Here\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    zip_file: str = \"/home/neon/Essentials/Jupyter/Hands-on-Project/a.zip\"\n",
    "    dest_file_path: str = \"/home/neon/Essentials/Jupyter/Hands-on-Project/a/\"\n",
    "    ack_filename: str = \"a.ack\"\n",
    "    hive_table_name: str = \"transaction_master\"\n",
    "\n",
    "    # Unzip the specified file and store in destination path\n",
    "    unzip_file(zip_file, dest_file_path)\n",
    "\n",
    "    # Check the destination that necessary file were present or not \n",
    "    if file_check(dest_file_path, ack_filename) <= 0:\n",
    "        sys.exit(1)    # Exit with failure status code\n",
    "\n",
    "    try:\n",
    "        # If the file not empty, convert the ack file to Data Frame\n",
    "        df = BigDataProcessing.get_data_frame(dest_file_path+'/'+ack_filename)\n",
    "        df = df.withColumn(\"filesize\", regexp_replace(col(\"filesize\"), r\"\\D+\", \"\"))\n",
    "    \n",
    "        # Collect the data\n",
    "        rows = df.collect()\n",
    "        list_of_files = [list(row.asDict().values()) for row in rows]\n",
    "\n",
    "        # Counter for skipping new table creation\n",
    "        count: int = 0\n",
    "        for file_props in list_of_files:\n",
    "            \n",
    "            # Check the sanity of extracted files\n",
    "            if ( file_check(dest_file_path, file_props[0]) == int(file_props[1])):\n",
    "                \n",
    "                # Check whether rows were matching or not\n",
    "                df = BigDataProcessing.get_data_frame(dest_file_path+'/'+file_props[0])\n",
    "                if (df.count() == file_props[2]):\n",
    "\n",
    "                    if count == 0:\n",
    "                        # Create new databases and then store the data into file\n",
    "                        BigDataProcessing.create_hive_tables()\n",
    "                        df.write.mode(\"overwrite\").saveAsTable(hive_table_name)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        df.write.mode(\"append\").saveAsTable(hive_table_name)\n",
    "\n",
    "        # Show hive table details\n",
    "        BigDataProcessing.show_hive_tables(hive_table_name)\n",
    "\n",
    "        # Retrive tables from hive and validate it\n",
    "        BigDataProcessing.retrive_hive_tables(hive_table_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Some exception occurred in main!\", e)\n",
    "                   \n",
    "    # Stopping the spark session which is initialized while getting data frame\n",
    "    BigDataProcessing.stop_spark_session()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19a74e-33d8-4c38-a2bc-3ea760b23439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.mode(\"append\").csv(dest_file_path+\"/Output\") if count != 0 else df.write.mode(\"overwrite\").csv(dest_file_path+\"/Output\")\n",
    "# count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
