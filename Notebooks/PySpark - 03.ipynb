{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e73e39f-5856-4d07-9f2e-aea5637b458f",
   "metadata": {},
   "source": [
    "## Handling Inconsistant data by cleaning it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c3954-5374-491e-bca8-576ecaafff44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Drop Null : Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2b60a-cc8a-405b-bd7d-339c90be1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    how = all (or) any\n",
    "    \n",
    "    axis = 0 - row\n",
    "           1 - column\n",
    "\n",
    "    alternate : df.na.drop()\n",
    "\"\"\"\n",
    "\n",
    "# To drop the null value rows in any column\n",
    "df.dropna()\n",
    "\n",
    "# To drop the null value rows in specific column\n",
    "df.dropna(subset=[\"col_name\"])\n",
    "\n",
    "# To drop\n",
    "df.dropna(how = '?', axis = ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448166a-2bca-47a1-9e00-d6aaca4bad6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Drop Duplicates : Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d782f87-4157-41ad-a8b7-9cc040dd3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   subset = we can mention column names\n",
    "\"\"\"\n",
    "\n",
    "# To drop the duplicates based on all the columns\n",
    "df.dropDuplicates()\n",
    "\n",
    "# To drop the duplicates based on the specified columns\n",
    "df.dropDuplicates(['column1', 'column2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd6327-2ee3-4f2c-8810-64ec5a4a7c39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fill : Empty or Unknown Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19e06f-2693-409e-9979-742d1dff9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    value = can be scalar (or) dictionary if different column should need different values\n",
    "\n",
    "    subset = we can mention column names for the scalar value\n",
    "\n",
    "    alternate : df.na.fill()\n",
    "\"\"\"\n",
    "\n",
    "# To replace all NaN values in the DataFrame with 0\n",
    "df.fillna(0)\n",
    "\n",
    "# To replace missing values in column1 with 0 and missing values in column2 with 'unknown'\n",
    "df.fillna({'column1': 0, 'column2': 'unknown'})\n",
    "\n",
    "# To replace missing values with 0 only in column1 and column2\n",
    "df.fillna(0, subset=['column1', 'column2'])\n",
    "\n",
    "# To fill\n",
    "df.fillna(value = ?, subset = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b40b0e-f091-45cb-a579-6123b317967a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cec01-cc5d-4a6b-bc7e-9de2c5cf5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    to_replace: The value or list of values to replace [ single value, a list of values, or a dictionary ]\n",
    "    \n",
    "    value: The new value or list of new values to replace to_replace values with. [ should be same count as to_replace ]\n",
    "   \n",
    "    subset: Optional. List of columns to apply the replacement to. If not specified, it applies to all columns.\n",
    "    \n",
    "    method: Optional. The method to use for replacement (e.g., 'pad', 'ffill', 'bfill').\n",
    "\n",
    "    alternate = df.na.replace()\n",
    "\"\"\"\n",
    "\n",
    "# Replacing the specified value to the new value\n",
    "DataFrame.replace(to_replace=None, value=None, subset=None, method=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4769cb1-d018-4565-ab25-e70573c84794",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143412c-07c2-4c54-b537-a5dcc43ef7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Don't know what is it : Z-scores, IQR (Interquartile Range), or custom thresholds.\n",
    "\n",
    "    Data which were beyond expectation and actual value.  We need to remove (or) transform this values to get correcct analytics/conclustion\n",
    "\"\"\"\n",
    "\n",
    "# Filter\n",
    "df.filter((col(\"value\") >= lower_bound) & (col(\"value\") <= upper_bound))\n",
    "\n",
    "# Caping (or) Normalizing value\n",
    "df.withColumn(\"col_name\", when(col(\"col_name\") < lower_bound, lower_bound).when(col(\"value\") > upper_bound, upper_bound).otherwise(col(\"col_name\")))\n",
    "\n",
    "# Imputation\n",
    "df.withColumn(\"value\", when((col(\"value\") < lower_bound) | (col(\"value\") > upper_bound), median_val).otherwise(col(\"value\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfaaae7-e38f-4324-a1f4-806e0799ca4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Non formated data like +91, $ and Male -> M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7bd981-a344-495d-b4cc-feb50a919002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, regexp_replace, when\n",
    "\n",
    "# Creating spark session object\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creating the data variable using row function\n",
    "data = [\n",
    "    Row(phone=\"+91-1234567890\", currency=\"$100\", gender=\"Male\"),\n",
    "    Row(phone=\"+44-9876543210\", currency=\"£200\", gender=\"Female\"),\n",
    "    Row(phone=\"+1-5555555555\", currency=\"€300\", gender=\"Non-binary\"),\n",
    "]\n",
    "\n",
    "# Converting the data to DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# printing the schema and data\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "\n",
    "# Removing non-numeric characters from the phone number\n",
    "df = df.withColumn(\"phone\", regexp_replace(col(\"phone\"), \"[^0-9-]\", \"\")) \n",
    "\n",
    "# Removing currency symbols\n",
    "df = df.withColumn(\"currency\", regexp_replace(col(\"currency\"), \"[$£€]\", \"\"))\n",
    "\n",
    "# Mapping gender to abbreviated form\n",
    "df = df.withColumn(\"gender\", when(col(\"gender\") == \"Male\", \"M\")\n",
    "                      .when(col(\"gender\") == \"Female\", \"F\")\n",
    "                      .otherwise(\"O\"))\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85299638-eb57-4cc9-ae92-bce1df617d25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Invalid Format and Data (or) Data Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f13015-281a-46df-97fb-4debd4b374b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the mentioned column value to the desired format and create new column\n",
    "df.withColumn(\"new_col_name\", to_date(col(\"col_name\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df.withColumn(\"standard_date\", \n",
    "    when(\n",
    "        col(\"date\").rlike(r\"\\d{4}-\\d{2}-\\d{2}\"), to_date(col(\"date\"), \"yyyy-MM-dd\")\n",
    "    ).when(\n",
    "        col(\"date\").rlike(r\"\\d{2}/\\d{2}/\\d{4}\"), to_date(col(\"date\"), \"dd/MM/yyyy\")\n",
    "    ).otherwise(\n",
    "        to_date(col(\"date\"), \"yyyy.MM.dd\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Normalize number format by removing thousands separators\n",
    "df = df.withColumn(\n",
    "    \"normalized_number\",\n",
    "    regexp_replace(col(\"number\"), r\"[^\\d.]\", \"\")  # Remove non-digit characters except decimal point\n",
    ")\n",
    "\n",
    "# Normalize to lowercase\n",
    "df.withColumn(\"normalized_response\", lower(col(\"response\")))\n",
    "\n",
    "\"\"\"\n",
    "    Invalid data can be handled using UDF or replacing the value with when and otherwise\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadbf27-d471-47ce-97aa-678a3117ae5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548db0e6-6772-45bb-8ccc-d2c23305f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the datatype of col_name to new data type\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"new_data_type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4c74e-2385-481c-aee9-26e336feef19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### When and Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811d886-25ee-40cd-91a7-67f1bc410613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"age_category\",\n",
    "    when(col(\"age\") < 18, \"Minor\")\n",
    "    .when(col(\"age\").between(18, 65), \"Adult\")\n",
    "    .otherwise(\"Senior\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107428e5-7a9e-4e52-a2c8-62b4e9250731",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb51e3-ed6b-46e5-8676-d89b743e6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    \\\\d matches any digit.\n",
    "    \\\\D matches any non-digit.\n",
    "    \\\\w matches any word character (alphanumeric plus underscore).\n",
    "    \\\\W matches any non-word character.\n",
    "    \\\\s matches any whitespace character.\n",
    "    \\\\S matches any non-whitespace character.\n",
    "\"\"\"\n",
    "\n",
    "# Just normal like as MySQL\n",
    "df.filter(col(\"code\").like(\"abc%\"))\n",
    "\n",
    "# Filter rows where the column contains digits using regular expression like\n",
    "df.filter(col(\"col_name\").rlike(\"\\\\d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b627e4b-a6ca-485d-9e63-c141074a83fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Use UDF to clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4005d-c00b-42d6-afe7-ca85ffeca8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def clean_date(date_str):\n",
    "    # Custom logic to handle different date formats\n",
    "    # Example: Handle 'MM/DD/YYYY' and 'YYYY-MM-DD'\n",
    "    # Return a standardized date string\n",
    "    return date_str  # Placeholder for actual logic\n",
    "\n",
    "clean_date_udf = udf(clean_date, StringType())\n",
    "\n",
    "df = df.withColumn(\"cleaned_date\", clean_date_udf(col(\"date\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7bc92-83e9-47f7-b7ad-d17df019b2d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Adding and Renaming and Removing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0a65c-6047-483c-b57e-e9c3ceeb18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Adding Columns: Use withColumn.\n",
    "    \n",
    "    Renaming Columns: Use withColumnRenamed.\n",
    "    \n",
    "    Removing Columns: Use drop.\n",
    "\"\"\"\n",
    "\n",
    "# Adding a new column based on existing columns withcolumn(new_col_name, some_condition)\n",
    "df.withColumn(\"new_col_name\", col(\"old_some_col\") + 1)  \n",
    "\n",
    "# Renaming the column\n",
    "df.withColumnRenamed(\"old_col_name\", \"new_col_name_for_same_col\")\n",
    "\n",
    "# Dropping existing column\n",
    "df.drop(\"new_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e8393-b34a-4576-b168-ff15b9a1691f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Other Topics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9738d0-4080-42af-81f6-433904f294ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. Logging\n",
    "\n",
    "    2. Monitoring\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
