{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e1177c-9de4-47d3-b067-377fd0097cdf",
   "metadata": {},
   "source": [
    "## References and Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fca38-195c-4df4-95ca-09ac8cce59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pyspark\n",
    "01. <a>https://youtu.be/EB8lfdxpirM?feature=shared</a>\n",
    "02. <a>https://www.udemy.com/course/introduction-to-python-for-big-data-engineering-with-pyspark/?couponCode=ST4MT73124</a>\n",
    "03. <a>https://www.youtube.com/watch?v=6MaZoOgJa84&list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_&index=1</a>\n",
    "04. <a>https://www.youtube.com/watch?v=AGgyf9bO_8M&list=PLlUZLZydkS7_8WnK8fMENmJFSfPwxw9Fi</a>\n",
    "\n",
    "#### Spark\n",
    "01. <a>https://www.youtube.com/watch?v=qU7u9wGB0JA&list=PLLa_h7BriLH27lOCmOOWhOuarb3HtSeaH</a>\n",
    "\n",
    "#### MySQL\n",
    "01. <a>https://youtu.be/5OdVJbNCSso?feature=shared</a>\n",
    "02. <a>https://youtu.be/7mz73uXD9DA?feature=shared</a>\n",
    "03. <a>https://www.javatpoint.com/mysql-partitioning</a>\n",
    "\n",
    "#### Linux\n",
    "01. <a>https://www.youtube.com/watch?v=4e669hSjaX8</a> (File and Directory Permissions)\n",
    "02. <a>https://www.youtube.com/watch?v=19WOD84JFxA</a> (User Management)\n",
    "03. <a>https://www.youtube.com/watch?v=bz0ZCUv5rYo</a> (Full Linux Tutorial : Process, File & Directories, User Management)\n",
    "\n",
    "#### DataBricks\n",
    "01. <a>https://www.youtube.com/watch?v=2-RIPNhhgHU&list=PL8zzpRdWG891m4LmFeVp-XkYh7ya-G6b6</a>\n",
    "\n",
    "#### Python\n",
    "01. <a>https://www.youtube.com/@Indently/videos</a>\n",
    "\n",
    "#### Others\n",
    "01. <a>https://www.youtube.com/watch?v=hdFk3EvL1ug&list=PLLa_h7BriLH1NK3hcydotCPgTh5xSWivM&index=2</a>\n",
    "\n",
    "#### Projects\n",
    "01. <a>https://www.youtube.com/watch?v=BlWS4foN9cY</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab073cb-3db2-4c61-9f7d-dc05d3f579a9",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590899ca-0109-45ef-9a37-61abb3a8329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "01. pyspark does not support DataSets because of type checking is not happens in python in compile time\n",
    "02. RDD and DataFrames in sufficient for the pyspark, and it has Sparkâ€™s Catalyst optimizer for the better performance\n",
    "03. Since PySpark does not support the Dataset API (which is available in Scala and Java), the PySpark DataFrame API has been designed to offer several advantages and features to make up for this limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e4d47-97e4-4e1c-be39-f840e4d32025",
   "metadata": {},
   "source": [
    "## Create SparkContext in Apache Spark version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ffc74-a561-444e-836f-c9ea9df59f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext object\n",
    "sc = SparkContext(appName=\"MySparkApplication\")\n",
    "\n",
    "# Print some info regarding the SparkContext\n",
    "sc\n",
    "\n",
    "# Need to stop it or else we can run another SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82626613-0b3d-4bc2-8e2a-8e4455946cb1",
   "metadata": {},
   "source": [
    "## Create SparkContext via SparkSession in Apache Spark version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed9134-5a9b-4051-bb2d-e58b671a0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark-Get-Started\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Create sparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Print some info regarding the SparkContext\n",
    "sc\n",
    "\n",
    "# Need to stop it or else we can run another SparkContext\n",
    "sc.stop() # or spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6a63d-046b-48ca-95c2-d2c67c5ec47d",
   "metadata": {},
   "source": [
    "## Creating RDD and DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec250b-1739-46d7-829b-fa0ce77c320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create RDD\n",
    "rdd = spark.sparkContext.parallelize([(1, 'Alice'), (2, 'Bob')])\n",
    "\n",
    "# Creating DataFrame from another RDD\n",
    "df = spark.createDataFrame(rdd, schema=['id', 'name'])\n",
    "\n",
    "# Creating DataFrame from other external source like CSV, JSON, Parquet\n",
    "# df = spark.read.csv(path = \"?\")\n",
    "\n",
    "# Creating new RDD by converting DataFrame to RDD\n",
    "rdd_new = df.rdd\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c351df-e60b-4410-8c10-46b824afb43c",
   "metadata": {},
   "source": [
    "## Functions and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a99132-7f57-4e05-a7e2-e0301b7ed665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RDD Functions\n",
    "\n",
    "01. collect()\n",
    "02. count()\n",
    "03. distinct()\n",
    "04. filter()\n",
    "05. map()            : Have the same number of output elements as input\n",
    "06. flatMap()\n",
    "07. sortByKey()      : sortByKey(bool), where bool represents ascending or desending.  False represents decending order.\n",
    "08. take()           : take(n)\n",
    "09. reduce()\n",
    "10. first()\n",
    "11. last()\n",
    "12. min()\n",
    "13. max()\n",
    "14. union()\n",
    "15. subtract()\n",
    "\n",
    "#### DF Functions\n",
    "01. show()\n",
    "02. printSchema()            : printSchema(truncate = False)\n",
    "03. select()\n",
    "04. columns()\n",
    "05. filter()\n",
    "06. where()\n",
    "07. distinct()\n",
    "08. show()\n",
    "09. alias()\n",
    "10. orderBy()\n",
    "11. dropDuplicates()\n",
    "12. withColumn()              : To add the column\n",
    "13. withColumnRenamed()\n",
    "14. drop()\n",
    "15. na.drop()                 : Drop the null value rows [na.drop(\"any\") vs na.drop(\"all\")]\n",
    "16. describe()                : Prints count, mean, max, min, stddev of a mentioned column\n",
    "17. groupBy()\n",
    "18. agg()\n",
    "19. count()\n",
    "20. isNull()\n",
    "21. cast()\n",
    "22. limit()\n",
    "23. toPandas()                : To convert DF to pandas DF\n",
    "24. createOrReplaceTempView   : To convert DF to the more SQL Table format\n",
    "\n",
    "#### Spark Functions\n",
    "1. createDataFrame()      : To create a DataFrame\n",
    "2. sparkContext()         : To create a SparkContext which used to do some works\n",
    "3. sql()                  : To use Spark SQL\n",
    "4. read()                 : To read various files from different formats\n",
    "5. write()                : To write various files in different formats\n",
    "6. range()                : To create a column with some values\n",
    "\n",
    "#### Pyspark imports\n",
    "1. from pyspark.sql import SparkSession, Row\n",
    "\n",
    "2. from pyspark.sql.functions import col, expr, concat, concat_ws, year, array_contains, count, desc, round, udf, countDistinct\n",
    "3. from pyspark.sql.functions import min, max, sumDistinct, avg, to_timestamp, date_format, regexp_replace, regexp_extract, lit, array\n",
    "4. from pyspark.sql.functions import explode, map_keys, map_values, split, array_contains, when, from_json, current_date, date_format\n",
    "5. from pyspark.sql.functions import to_date\n",
    "\n",
    "6. from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, DateType, BooleanType\n",
    "7. from pyspark.sql.types import MapType"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
