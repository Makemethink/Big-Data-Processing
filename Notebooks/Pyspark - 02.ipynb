{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44091052-558e-48d2-9f2e-3842bb30fe28",
   "metadata": {},
   "source": [
    "## Pyspark Programs - Focusing DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be0b01-aade-4c64-afe0-d617775bb2b6",
   "metadata": {},
   "source": [
    "#### Sample Program - 01 :  createDataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518a8ee6-6908-4eee-a083-7d58997c63b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 11:50:17 WARN Utils: Your hostname, neon-HP-Pavilion-Gaming-Laptop-15-ec1xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlo1)\n",
      "24/08/23 11:50:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/23 11:50:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |name |\n",
      "+---+-----+\n",
      "|1  |one  |\n",
      "|2  |two  |\n",
      "|3  |three|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+------+\n",
      "|id |name  |\n",
      "+---+------+\n",
      "|1  |Swami |\n",
      "|2  |Nathan|\n",
      "|3  |Rahul |\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----------+\n",
      "|id |name      |\n",
      "+---+----------+\n",
      "|1  |spark     |\n",
      "|2  |linux     |\n",
      "|3  |sql       |\n",
      "|4  |databricks|\n",
      "|5  |python    |\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    createDataFrame() -> With ( with & without types ) & Without schema\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"createDF\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data_one = [(1, \"one\"), (2, \"two\"), (3, \"three\")]\n",
    "data_two = [(1, \"Swami\"), (2, \"Nathan\"), (3, \"Rahul\")]\n",
    "data_three = [{'id': 1, 'name': \"spark\"},\n",
    "            {'id': 2, 'name': \"linux\"},\n",
    "            {'id': 3, 'name': \"sql\"},\n",
    "            {'id': 4, 'name': \"databricks\"},\n",
    "            {'id': 5, 'name': \"python\"}]\n",
    "\n",
    "schema_one = StructType([StructField(name = 'id', dataType = IntegerType()),\n",
    "                         StructField(name = 'name', dataType = StringType())])\n",
    "\n",
    "schema_two = [\"id\", \"name\"]\n",
    "\n",
    "df_one = spark.createDataFrame(data = data_one, schema = schema_one)\n",
    "df_two = spark.createDataFrame(data = data_two, schema = schema_two)\n",
    "df_three = spark.createDataFrame(data = data_three)\n",
    "\n",
    "df_one.printSchema()\n",
    "df_one.show(5, truncate = False)\n",
    "df_two.printSchema()\n",
    "df_two.show(5, truncate = False)\n",
    "df_three.printSchema()\n",
    "df_three.show(5, truncate = False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722b4a8-eafd-4295-8ef2-d3ead1e57454",
   "metadata": {},
   "source": [
    "#### Sample Program - 02 : Read from external source (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837ba89-d158-448f-887a-9a34925fc720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read CSV file with csv() and format() methods\n",
    "\n",
    "    -> DataFrameReader = spark.read\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"csv\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Extending (incrementally we can add)\n",
    "schema = StructType().add(field='name', data_type=StringType()) \\\n",
    "                     .add(field='age', data_type=IntegerType()) \\\n",
    "                     .add(field='gender', data_type=StringType()) \\\n",
    "                     .add(field='salary', data_type=IntegerType())\n",
    "\n",
    "# The path variable can get array also.  So, we can give multiple files at same time and load into single DF.\n",
    "# If there is a folder with csv files, then specifying the folder itself fine to load all the csv with in it.\n",
    "df_one = spark.read.csv(path = \"./Dependencies/persons.csv\", header = True, schema = schema)\n",
    "\n",
    "display(df_one)\n",
    "df_one.show(5, truncate=False)\n",
    "df_one.printSchema()\n",
    "\n",
    "df_two = spark.read.format('csv').option('header', True).option('inferSchema', True).load(path = \"persons.csv\")\n",
    "\n",
    "display(df_two)\n",
    "df_two.show(5, truncate=False)\n",
    "df_two.printSchema()\n",
    "\n",
    "df_three = spark.read.format('csv').option('header', True).option('inferSchema', True).load(path = \"persons.csv\")\n",
    "\n",
    "display(df_three)\n",
    "df_three.show(5, truncate=False)\n",
    "df_three.printSchema()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906cd60-55d3-447c-ae93-33a25d47bbaa",
   "metadata": {},
   "source": [
    "#### Sample Program - 03 : Write to external destination (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1adb1ab-0e34-4a53-b68d-460be2abfd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|id |name      |\n",
      "+---+----------+\n",
      "|1  |spark     |\n",
      "|2  |linux     |\n",
      "|3  |sql       |\n",
      "|4  |databricks|\n",
      "|5  |python    |\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Write into CSV file\n",
    "\n",
    "    -> DataFrameWriter = spark.write\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"csv\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [{'id': 1, 'name': \"spark\"},\n",
    "        {'id': 2, 'name': \"linux\"},\n",
    "        {'id': 3, 'name': \"sql\"},\n",
    "        {'id': 4, 'name': \"databricks\"},\n",
    "        {'id': 5, 'name': \"python\"}]\n",
    "\n",
    "df = spark.createDataFrame(data = data)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "# If you does not want more partitional then we can use repartition function and give 1 as a input\n",
    "df.write.csv(path = \"./Dependencies/lang.csv\", header = True, mode=\"ignore\")\n",
    "\n",
    "# Modes : append | overwrite | error | ignore\n",
    "# Another way to attain the same functionality\n",
    "# df.write.format(\"csv\").mode('overwrite').save(\"lang.csv\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd2739-0b92-412a-9ddc-240316f1e11c",
   "metadata": {},
   "source": [
    "#### Sample Program - 04 : Read from external source (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82816d8e-a9b8-4fad-abeb-73f3d548cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 11:15:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+----------------------+------+--------+\n",
      "|category       |id |name                  |price |quantity|\n",
      "+---------------+---+----------------------+------+--------+\n",
      "|Electronics    |1  |iPhone 12             |899.99|10      |\n",
      "|Clothing       |2  |Nike Air Max 90       |119.99|25      |\n",
      "|Home Appliances|3  |KitchenAid Stand Mixer|299.99|5       |\n",
      "|Books          |4  |The Great Gatsby      |12.99 |50      |\n",
      "|Beauty         |5  |L'Oreal Paris Mascara |9.99  |100     |\n",
      "+---------------+---+----------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+----------------------+------+--------+\n",
      "|category       |id |name                  |price |quantity|\n",
      "+---------------+---+----------------------+------+--------+\n",
      "|Electronics    |1  |iPhone 12             |899.99|10      |\n",
      "|Clothing       |2  |Nike Air Max 90       |119.99|25      |\n",
      "|Home Appliances|3  |KitchenAid Stand Mixer|299.99|5       |\n",
      "|Books          |4  |The Great Gatsby      |12.99 |50      |\n",
      "|Beauty         |5  |L'Oreal Paris Mascara |9.99  |100     |\n",
      "+---------------+---+----------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Read JSON file with read.json() and format() methods\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"csv\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Single Line Json file\n",
    "df = spark.read.json(path = \"./Dependencies/products_singleline.json\")\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "# Multi Line Json file (with pretty print)\n",
    "df1 = spark.read.json(path = \"./Dependencies/products_multiline.json\", multiLine = True)\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "# Wild card (all json files inside the folder)\n",
    "# Multi json files as an array loading at once\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57868b11-579b-4f5d-9aff-3c31c73107af",
   "metadata": {},
   "source": [
    "#### Sample Program - 05 : Write to external destination (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9eae694-26af-41dc-ba4e-b094b48faa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 11:21:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1| swami|\n",
      "|  2|nathan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Write result into JSON file with writer object\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"csv\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [(1, 'swami'), (2, 'nathan')]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "display(df)\n",
    "df.show()\n",
    "\n",
    "df.write.json(\"./Dependencies/json_output.json\", mode = 'ignore')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73b712-38cd-4acb-838d-a5d7b19ece49",
   "metadata": {},
   "source": [
    "#### Sample Program - 06 : Read from external source (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a89c6009-970c-47b8-9898-0336d0b28003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 11:27:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "|         Sports|  6|            Yoga Mat| 29.99|      30|\n",
      "|    Electronics|  7| Samsung 4K Smart TV|799.99|       8|\n",
      "|       Clothing|  8|        Levi's Jeans| 49.99|      15|\n",
      "|Home Appliances|  9|Dyson Vacuum Cleaner|399.99|       3|\n",
      "|          Books| 10| Harry Potter Series| 15.99|      20|\n",
      "|         Beauty| 11|        MAC Lipstick| 16.99|      75|\n",
      "|         Sports| 12|Adidas Running Shoes| 59.99|      22|\n",
      "|    Electronics| 13|       PlayStation 5|499.99|      12|\n",
      "|       Clothing| 14|   Hooded Sweatshirt| 34.99|      10|\n",
      "|Home Appliances| 15|        Coffee Maker| 89.99|       7|\n",
      "|          Books| 16|To Kill a Mocking...|  9.99|      15|\n",
      "|         Beauty| 17|        Skincare Set| 49.99|      50|\n",
      "|         Sports| 18|           Yoga Ball| 19.99|      18|\n",
      "|    Electronics| 19|Sony Noise-Cancel...|299.99|       6|\n",
      "|       Clothing| 20|        Puma T-shirt| 19.99|      40|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, id: bigint, name: string, price: double, quantity: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"csv\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Wildcard reading of the parquet file also we can do\n",
    "df = spark.read.parquet('./Dependenciesproducts_parquet.parquet/part-00000-1f07d3ad-5b1e-4437-add7-979e91115b3c-c000.snappy.parquet')\n",
    "df.show()\n",
    "df.printSchema()\n",
    "display(df)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3eeff-6904-448f-be44-b4118d380939",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 07 : Write to external destination (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48559d25-f0f8-45d7-a268-e5350f8b289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Write result into Parquet file with writer object\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"csv\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [(1, 'swami'), (2, 'nathan')]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "display(df)\n",
    "df.show()\n",
    "\n",
    "\n",
    "df.write.parquet(\"json_output.parquet\", mode = 'ignore')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755963b-74ec-4285-9c21-442c50426cd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 08 : show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bbe0f7a-c647-4f62-aedd-fd561edbc96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 12:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Apach...|\n",
      "|  2|linux...|\n",
      "|  3|SQL s...|\n",
      "+---+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+----------------------------------------------------+\n",
      "|id |name                                                |\n",
      "+---+----------------------------------------------------+\n",
      "|1  |Apache spark is alternate to the MapReduce in Hadoop|\n",
      "|2  |linux is a kernal not an Operating System           |\n",
      "|3  |SQL stands for Structured Query Language            |\n",
      "+---+----------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "-RECORD 0--------------------\n",
      " id   | 1                    \n",
      " name | Apache spark is a... \n",
      "-RECORD 1--------------------\n",
      " id   | 2                    \n",
      " name | linux is a kernal... \n",
      "-RECORD 2--------------------\n",
      " id   | 3                    \n",
      " name | SQL stands for St... \n",
      "-RECORD 3--------------------\n",
      " id   | 4                    \n",
      " name | databricks compan... \n",
      "-RECORD 4--------------------\n",
      " id   | 5                    \n",
      " name | python is a progr... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Operations of show()\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"csv\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [{'id': 1, 'name': \"Apache spark is alternate to the MapReduce in Hadoop\"},\n",
    "        {'id': 2, 'name': \"linux is a kernal not an Operating System\"},\n",
    "        {'id': 3, 'name': \"SQL stands for Structured Query Language\"},\n",
    "        {'id': 4, 'name': \"databricks company started by the one who invented Spark\"},\n",
    "        {'id': 5, 'name': \"python is a programming language written before java\"}]\n",
    "\n",
    "df = spark.createDataFrame(data=data)\n",
    "\n",
    "# show prints 20 character as and 20 rows as a default\n",
    "# show can get integers character and boolean character for the truncate argument\n",
    "# show have n property to get no of rows to print\n",
    "# show can able to print the table vertically using vertical = True parameter\n",
    "\n",
    "df.show(n = 3, truncate = 8)\n",
    "df.show(n = 3, truncate = False)\n",
    "df.show(vertical = True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9466ce-d668-4851-95b1-4696f46b2193",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 09 : withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83995ac9-a5fd-42af-8d75-6f70d69e4f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 13:14:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- country: string (nullable = false)\n",
      " |-- this_month_salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+-------+-----------------+\n",
      "| id|      name|salary|country|this_month_salary|\n",
      "+---+----------+------+-------+-----------------+\n",
      "|  1|     spark| 60000|  india|            60000|\n",
      "|  2|     linux| 50000|  india|            50000|\n",
      "|  3|       sql| 40000|  india|            40000|\n",
      "|  4|databricks| 30000|  india|            30000|\n",
      "|  5|    python|100000|  india|           100000|\n",
      "+---+----------+------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Transformation\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"createDF\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [{'id': 1, 'name': \"spark\", 'salary': 30_000},\n",
    "        {'id': 2, 'name': \"linux\", 'salary': 25_000},\n",
    "        {'id': 3, 'name': \"sql\", 'salary': 20_000},\n",
    "        {'id': 4, 'name': \"databricks\", 'salary': 15_000},\n",
    "        {'id': 5, 'name': \"python\", 'salary': 50_000}]\n",
    "\n",
    "df = spark.createDataFrame(data = data)\n",
    "\n",
    "# Taking an RDD using withColumn and changing the long data type to Integer\n",
    "df1 = df.withColumn(colName = 'id', col=col('id').cast('Integer')) \\\n",
    "        .withColumn(colName = 'salary', col=col('salary').cast('Integer')) \\\n",
    "        .withColumn(colName = 'salary', col=col('salary') * 2) \n",
    "\n",
    "# If colName is already exists it will update the value from the expression given in col parameter else it will create new\n",
    "df1 = df1.withColumn(colName = 'country', col = lit('india'))\n",
    "\n",
    "df1 = df1.withColumn(colName = 'this_month_salary', col = col('salary'))\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show(5)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700ecb2-5776-465f-b418-dd633ada200b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 10 : withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a6371a0-e969-44f3-8a18-59fcc4554ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 14:04:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+\n",
      "| id|      name|month_salary|\n",
      "+---+----------+------------+\n",
      "|  1|     spark|       30000|\n",
      "|  2|     linux|       25000|\n",
      "|  3|       sql|       20000|\n",
      "|  4|databricks|       15000|\n",
      "|  5|    python|       50000|\n",
      "+---+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Transformation\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"createDF\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [{'id': 1, 'name': \"spark\", 'salary': 30_000},\n",
    "        {'id': 2, 'name': \"linux\", 'salary': 25_000},\n",
    "        {'id': 3, 'name': \"sql\", 'salary': 20_000},\n",
    "        {'id': 4, 'name': \"databricks\", 'salary': 15_000},\n",
    "        {'id': 5, 'name': \"python\", 'salary': 50_000}]\n",
    "\n",
    "df = spark.createDataFrame(data = data)\n",
    "\n",
    "df1 = df.withColumnRenamed('salary', 'month_salary')\n",
    "\n",
    "df1.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19850b58-8685-4aee-8496-a76217b85783",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 11 : Schema Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b59b26b-604e-46d4-94ce-d6dba520e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 14:15:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- First: string (nullable = true)\n",
      " |    |-- Last: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Schema Structures\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"createDF\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [{'id': 1, 'name': \"spark\", 'salary': 30_000},\n",
    "        {'id': 2, 'name': \"linux\", 'salary': 25_000},\n",
    "        {'id': 3, 'name': \"sql\", 'salary': 20_000},\n",
    "        {'id': 4, 'name': \"databricks\", 'salary': 15_000},\n",
    "        {'id': 5, 'name': \"python\", 'salary': 50_000}]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name = 'id', dataType = IntegerType()),\n",
    "    StructField(name = 'name', dataType = StringType()),\n",
    "    StructField(name = 'salary', dataType = IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "\n",
    "# Complex data types (Multivalued data in SQL)\n",
    "\n",
    "data_one = [{'id': 1, 'name': ('apache', 'spark'), 'salary': 30_000},\n",
    "            {'id': 2, 'name': ('os', 'linux'), 'salary': 25_000},\n",
    "            {'id': 3, 'name': ('sql', 'Mysql'), 'salary': 20_000},\n",
    "            {'id': 4, 'name': ('UI', 'Databricks'), 'salary': 15_000},\n",
    "            {'id': 5, 'name': ('lang', 'python'), 'salary': 50_000}]\n",
    "\n",
    "structName = StructType([StructField('First', StringType()), StructField('Last', StringType())])\n",
    "\n",
    "schema_one = StructType([\n",
    "    StructField(name = 'id', dataType = IntegerType()),\n",
    "    StructField(name = 'name', dataType = structName),\n",
    "    StructField(name = 'salary', dataType = IntegerType())\n",
    "])\n",
    "\n",
    "df_one = spark.createDataFrame(data = data_one, schema = schema_one)\n",
    "df_one.printSchema()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9bfa2-0ed8-4296-b8d9-53c2eb3eadac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 12 : ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "726a72e3-c44f-42a5-a401-24322e62fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------------+------+------------+-------+\n",
      "|    extra| id|            name|salary|first_sector|altered|\n",
      "+---------+---+----------------+------+------------+-------+\n",
      "|[1, 2, 3]|  1| {apache, spark}| 30000|           1| [1, 1]|\n",
      "|[3, 4, 5]|  2|     {os, linux}| 25000|           3| [2, 3]|\n",
      "|[6, 7, 8]|  3|    {sql, Mysql}| 20000|           6| [3, 6]|\n",
      "|[9, 1, 0]|  4|{UI, Databricks}| 15000|           9| [4, 9]|\n",
      "|[1, 1, 1]|  5|  {lang, python}| 50000|           1| [5, 1]|\n",
      "+---------+---+----------------+------+------------+-------+\n",
      "\n",
      "root\n",
      " |-- extra: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- _1: string (nullable = true)\n",
      " |    |-- _2: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- first_sector: long (nullable = true)\n",
      " |-- altered: array (nullable = false)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    ArrayType\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "from pyspark.sql.functions import col, array\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"ArrayType\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [{'id': 1, 'name': ('apache', 'spark'), 'salary': 30_000, 'extra': [1,2,3]},\n",
    "        {'id': 2, 'name': ('os', 'linux'), 'salary': 25_000, 'extra': [3,4,5]},\n",
    "        {'id': 3, 'name': ('sql', 'Mysql'), 'salary': 20_000, 'extra': [6,7,8]},\n",
    "        {'id': 4, 'name': ('UI', 'Databricks'), 'salary': 15_000, 'extra': [9,1,0]},\n",
    "        {'id': 5, 'name': ('lang', 'python'), 'salary': 50_000, 'extra': [1,1,1]}]\n",
    "\n",
    "structName = StructType([StructField('First', StringType()), StructField('Last', StringType())])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name = 'id', dataType = IntegerType()),\n",
    "    StructField(name = 'name', dataType = structName),\n",
    "    StructField(name = 'salary', dataType = IntegerType()),\n",
    "    StructField(name = 'extra', dataType = ArrayType(IntegerType()))])\n",
    "\n",
    "df = spark.createDataFrame(data = data)\n",
    "\n",
    "df1= df.withColumn(colName = 'first_sector', col=col('extra')[0])\n",
    "\n",
    "# To combine two separate column to single multivalued column\n",
    "df1 = df1.withColumn(colName = 'altered', col=array(col('id'), col('first_sector')))\n",
    "\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ade0d-b1a3-4bd2-9abf-bdbb30fed346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 13 : Other functions of ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd9bd9-41ca-409d-96af-8217aa9f97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    -> explode()\n",
    "    -> split()\n",
    "    -> arry()\n",
    "    -> array_contains()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab991b-6438-4a5f-9262-d73bcec07d32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 14 : MapType and other funtions of MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422737f-4ac6-46b4-a5cc-e1df2b047df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    MapType Column\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    -> map_keys()\n",
    "    -> map_values()\n",
    "    -> explode()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86a0d7-1740-415c-b1b4-7268343a6262",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 15 : Row and Column Class (col) and how to access it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5a15b918-324e-4ab0-9d44-ab2f0be7fa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 18:08:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n",
      "|  name|salary|age|\n",
      "+------+------+---+\n",
      "|naruto|  3000| 23|\n",
      "|sasuke|  2000| 24|\n",
      "+------+------+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|itachi| 23|\n",
      "|nagato| 22|\n",
      "+------+---+\n",
      "\n",
      "+------------+-------------------+\n",
      "|        name|               prop|\n",
      "+------------+-------------------+\n",
      "|Yellow Flash|{30, teleportation}|\n",
      "|  White Fang|      {40, Unknown}|\n",
      "+------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- age: long (nullable = true)\n",
      " |    |-- power: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Row\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Way 1 of using Row\n",
    "row1 = Row(name = 'naruto', salary = 3000, age = 23)\n",
    "row2 = Row(name = 'sasuke', salary = 2000, age = 24)\n",
    "\n",
    "data = [row1, row2]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# Another Way (2)\n",
    "person = Row('name', 'age')\n",
    "\n",
    "row1 = person(\"itachi\", 23)\n",
    "row2 = person(\"nagato\", 22)\n",
    "\n",
    "df_one = spark.createDataFrame(data = [row1, row2])\n",
    "df_one.show()\n",
    "\n",
    "# Another way (3)\n",
    "\n",
    "data_last = [\n",
    "    Row(name = \"Yellow Flash\", prop = Row(age = 30, power = \"teleportation\")),\n",
    "    Row(name = \"White Fang\", prop = Row(age = 40, power = \"Unknown\"))\n",
    "]\n",
    "\n",
    "df_last = spark.createDataFrame(data = data_last)\n",
    "df_last.show()\n",
    "df_last.printSchema()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3b0fd-4d0a-438e-8eea-bceac44c4732",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sample Program - 16 : when() and otherwise() : Similar to CASE statement in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed5bfe1-12d1-452c-ba15-59ed1caca9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|gender| id|      name|salary|\n",
      "+------+---+----------+------+\n",
      "|     M|  1|     spark| 30000|\n",
      "|     F|  2|     linux| 25000|\n",
      "|      |  3|       sql| 20000|\n",
      "|     F|  4|databricks| 15000|\n",
      "|     M|  5|    python| 50000|\n",
      "+------+---+----------+------+\n",
      "\n",
      "+---+----------+------+-------+\n",
      "| id|      name|salary| Gender|\n",
      "+---+----------+------+-------+\n",
      "|  1|     spark| 30000|   Male|\n",
      "|  2|     linux| 25000| Female|\n",
      "|  3|       sql| 20000|Unknown|\n",
      "|  4|databricks| 15000| Female|\n",
      "|  5|    python| 50000|   Male|\n",
      "+---+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"when_otherwise\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [{'id': 1, 'name': \"spark\", 'salary': 30_000, \"gender\": 'M'},\n",
    "        {'id': 2, 'name': \"linux\", 'salary': 25_000, \"gender\": 'F'},\n",
    "        {'id': 3, 'name': \"sql\", 'salary': 20_000, \"gender\": ''},\n",
    "        {'id': 4, 'name': \"databricks\", 'salary': 15_000, \"gender\": 'F'},\n",
    "        {'id': 5, 'name': \"python\", 'salary': 50_000, \"gender\": 'M'}]\n",
    "\n",
    "df = spark.createDataFrame(data = data)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df1 = df.select(df.id, df.name, df.salary, \n",
    "                when(df.gender=='M', 'Male')\n",
    "               .when(df.gender=='F', 'Female')\n",
    "               .otherwise('Unknown').alias('Gender'))\n",
    "df1.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c50350-487d-4d90-ac90-63eee7dcaa39",
   "metadata": {},
   "source": [
    "#### Sample Program - 17 : Other column functions focusing DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6a6ad-7305-4fd2-b70d-3053c84eba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    df.columnname.method()\n",
    "    \n",
    "    -> alias()\n",
    "    -> asc()\n",
    "    -> desc()\n",
    "    -> cast()\n",
    "    -> like()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    -> where() / filter()\n",
    "    -> distinct() / dropDuplicates()\n",
    "    -> orderBy() / sort()  --- default ascending order ---- df.sort(df.col_one, df.col_two.desc())\n",
    "    -> union() / unionAll() / unionByName()\n",
    "    -> groupBy()  --- have many things, max(), min(), count(), agg()\n",
    "    -> select() --- you can pass column name as list, df.columnname, df['columnname'] or col() | select * | [ col for col in df.columns ]\n",
    "    -> join() --- join(another table, on condition, what join)\n",
    "    -> pivot() --- ungrouping \n",
    "    -> stack() --- unpivoting\n",
    "    -> fill() / fillna() --- changes null values to some other mentioned value\n",
    "    -> sample()\n",
    "    -> collect() --- returns a array / list not df or rdd (so it was an action)\n",
    "    -> transform() / pyspark.sql.functions.transform() ---\n",
    "    -> createOrReplaceTempView() / createOrReplaceGlobalTempView() [used across all section as global]\n",
    "    -> from_json()  --- we can convert the json data column to MapType() or Structure Type()\n",
    "    -> to_json() --- \n",
    "    -> json_tuple()\n",
    "    -> get_json_object()\n",
    "    -> current_date() / date_format() / to_date() \n",
    "    -> datediff() / months_between() / add_months() / date_add() / year() / month()\n",
    "    -> current_timestamp(), to_timestamp(), hour(), minute(), second()\n",
    "    -> row_number() --- adds the row\n",
    "    -> rank() / dense_rank() --- based upon the column specified it assign the value (dense rank don't skip)\n",
    "    -> over()\n",
    "    \n",
    "    Aggregate methods:\n",
    "    -----------------\n",
    "    -> approx_count_distinct() / avg() / collect_list() / collect_set() / count() / countDistinct()\n",
    "    \n",
    "\n",
    "    -> spark.catalog.currentDatabase()\n",
    "    -> spark.catalog.listTables()\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
