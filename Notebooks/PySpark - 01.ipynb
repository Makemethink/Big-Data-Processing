{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6fc708-0e8a-48e2-ae11-3c418ce96ec4",
   "metadata": {},
   "source": [
    "## Programs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2a644-1094-4dca-b7a5-13ba4aebd970",
   "metadata": {},
   "source": [
    "### Sample Program - 00 : parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2eab3-72f9-467e-bbc6-36663e0efdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a pyspark session and naming the app and giving the configurations\n",
    "spark = SparkSession.builder.appName(\"Python\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "# Creating RDD and converting it as a DataFrame in single line\n",
    "df = spark.sparkContext.parallelize([(\"Alice\", 25), (\"Bob\", 23), (\"Charlie\", 35)]).toDF([\"Name\", \"Age\"])\n",
    "\n",
    "# Print the DF\n",
    "df.show()\n",
    "\n",
    "# Stop the spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25aa7b2-6cbd-4573-a40f-1c7218ba7714",
   "metadata": {},
   "source": [
    "### Sample Program - 01 : createDataFrame() from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425b5c4-5384-48cc-8a04-e2eba876df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which create a DataFrame from python list and show, as well as print the structure of DataFrame\n",
    "\"\"\"\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark-Get-Started\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Creating a List\n",
    "data = [(\"Alice\", 25), (\"Bob\", 23), (\"Charlie\", 35)]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Print some info\n",
    "df.show(3)          # Default 20 rows\n",
    "df.printSchema()    # Print the DF schema\n",
    "\n",
    "# Stoping the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31487aa6-411b-4de1-bdf4-975e88cd8432",
   "metadata": {},
   "source": [
    "### Sample Program - 02 : createDataFrame() from python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632209b-cec4-46ab-984b-1236f644ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which create a DataFrame from python list\n",
    "\"\"\"\n",
    "\n",
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a pyspark session and naming the app and giving the configurations\n",
    "spark = SparkSession.builder.appName(\"Python\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "# Creating DataFrame from the python list directly\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "\n",
    "# Stoping the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999ee6c-d8bf-47b3-829b-9d26c6fa44f0",
   "metadata": {},
   "source": [
    "### Sample Program - 03 : parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041be29-a7db-422b-99f6-0c3ca98ec034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which create a RDD using parallelize() and uses collect() action\n",
    "\"\"\"\n",
    "\n",
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a pyspark session and naming the app and giving the configurations\n",
    "spark = SparkSession.builder.appName(\"Python\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "# Creating an RDD\n",
    "mydata = spark.sparkContext.parallelize([(1,2),(3,4),(5,6),(7,8),(9,10)])\n",
    "\n",
    "# Using the action\n",
    "mydata.collect()\n",
    "\n",
    "# Stoping the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48d7d7-ee1a-4056-a875-119782ca8b35",
   "metadata": {},
   "source": [
    "### Sample Program - 04 : pyspark.SparkContext() and count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d66e2-fa04-478d-85a4-7fa695847bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which create a RDD using parallelize() using SparkContext without SparkSession and uses count() action\n",
    "\"\"\"\n",
    "\n",
    "# Importing necessary packages\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(\"local\", \"count app\")\n",
    "\n",
    "# Creating a RDD using parallelize\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "\n",
    "# Using the count() action\n",
    "counts = words.count()\n",
    "\n",
    "# Print the output\n",
    "print(f\"The total count was : {counts}\")\n",
    "\n",
    "# Stoping the SparkSession\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff98a76-c3cb-4e52-ace3-19f56e8968c8",
   "metadata": {},
   "source": [
    "### Sample Program - 05 : DataFrame using external source (.csv file) via format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787f153b-4d31-4c67-968f-0a8e9db32e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 01:10:16 WARN Utils: Your hostname, neon-HP-Pavilion-Gaming-Laptop-15-ec1xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlo1)\n",
      "24/08/09 01:10:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/09 01:10:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/neon/Essentials/Jupyter/path/to/your/file.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.some.config.option\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome-value\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Creating a DataFrame by reading data from a .csv file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath/to/your/file.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Provide the correct path to your CSV file\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print the DataFrame\u001b[39;00m\n\u001b[1;32m     16\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Only shows 5 rows\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/neon/Essentials/Jupyter/path/to/your/file.csv."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Loading data from external source .csv file using format() and creating DataFrame\n",
    "\"\"\"\n",
    "\n",
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a Spark session and naming the app\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creating a DataFrame by reading data from a .csv file\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .options(header='true', inferSchema='true') \\\n",
    "    .load(\"path/to/your/file.csv\")  # Provide the correct path to your CSV file\n",
    "\n",
    "# Print the DataFrame\n",
    "df.show(5)  # Only shows 5 rows\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Stoping the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95265b7-0aad-4914-9859-77d30177e65a",
   "metadata": {},
   "source": [
    "### Sample Program - 06 : first(), take(), foreach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13128a-a1a4-4062-93d4-8963527e044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which create a RDD and retrive first and N elements\n",
    "\"\"\"\n",
    "\n",
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a pyspark session and naming the app and giving the configurations\n",
    "spark = SparkSession.builder.appName(\"Python\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    "\n",
    "# Converting the data to RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Taking first element of the RDD and printing it\n",
    "first_element = rdd.first()\n",
    "print(f'The first element was = {first_element}')\n",
    "\n",
    "# Taking some number of element from RDD and printing it\n",
    "elements_taken = rdd.take(2)\n",
    "print(f'The elements taken was = {elements_taken}')\n",
    "\n",
    "# Printing all the elements using lambda and foreach\n",
    "rdd.foreach(lambda x: print(x))\n",
    "\n",
    "# Stoping the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b516c-b547-46ec-9624-22ada49783d3",
   "metadata": {},
   "source": [
    "### Sample Program - 07 : map, filter, sort and reducebykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d82a1d-eeb6-4222-8fa6-1402cbaf0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which create a RDD and use map, reduce and reducebykey transformation\n",
    "\"\"\"\n",
    "\n",
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a pyspark session and naming the app and giving the configurations\n",
    "spark = SparkSession.builder.appName(\"Python\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 23), (\"Charlie\", 35), (\"Bob\", 22)]\n",
    "\n",
    "# Converting the data to RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# New RDD was created, because we used the map transformation map here\n",
    "mapped_rdd = rdd.map(lambda x: (x[0].upper(), x[1]))\n",
    "\n",
    "# New RDD was created, because we used the map transformation filter here\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] < 30)\n",
    "\n",
    "# New RDD was created, because we used the map transformation reduceByKey here\n",
    "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# New RDD was created, because we used the map transformation sortBy here\n",
    "sorted_rdd = rdd.sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "# This is a RDD to print this we need to do collect() \n",
    "print(mapped_rdd)\n",
    "print(filtered_rdd)\n",
    "print(reduced_rdd)\n",
    "print(sorted_rdd)\n",
    "\n",
    "# Printing the actual data using collect() action\n",
    "print(f'The final mapped data was : {mapped_rdd.collect()}')\n",
    "print(f'The final filtered data was : {filtered_rdd.collect()}')\n",
    "print(f'The final reduced data was : {reduced_rdd.collect()}')\n",
    "print(f'The final sorted data was : {sorted_rdd.collect()}')\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488dcaf-4585-4a12-8f88-89c8b3f07fa2",
   "metadata": {},
   "source": [
    "### Sample Program - 08 : Saving RDD result using saveAsTextFile() and retrieving using sc.textFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86501620-9b90-40c8-bc7a-ce357e5e889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which stores RDD into file and retrieve the same and print it\n",
    "\"\"\"\n",
    "\n",
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark-Get-Started\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Creating a RDD using parallelize\n",
    "rdd = spark.sparkContext.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "\n",
    "# Saving the RDD in Text file\n",
    "rdd.saveAsTextFile(\"./Dependencies/output.txt\")\n",
    "\n",
    "# Retrieving the same RDD by reading from Text file\n",
    "rdd_text = spark.sparkContext.textFile(\"./Dependencies/output.txt\")\n",
    "\n",
    "# Printing the content of the RDD file\n",
    "print(f'The content of the file : {rdd_text.collect()}')\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb3ef-2e3f-4feb-a3db-23ced47e4d69",
   "metadata": {},
   "source": [
    "### Sample Program - 09 : Some Operations on RDD as well as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604d0ad-05ef-428a-8ce3-f317981eccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The program which perform some operations on RDD and DF\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DF-Creation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Create an RDD (spark session initiated sparkContext)\n",
    "rdd = spark.sparkContext.textFile(\"./Dependencies/data.txt\")\n",
    "\n",
    "# Flat the RDD, make each word in each sentence as a list (like 1D array)\n",
    "# Get Mapped to the RDD here\n",
    "# Remove duplicates as same key will be processed here\n",
    "# Sort in decending order carried out here\n",
    "\n",
    "result_rdd = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda x, y: x + y) \\\n",
    "             .sortBy(lambda x: x[1], ascending = False)    \n",
    "\n",
    "# Printing the resultant top 10 words\n",
    "print(f\"The resultant set was : {result_rdd.take(10)}\")\n",
    "\n",
    "# Create an DF (spark session initiated sqlContext)\n",
    "df = spark.read.text(\"/home/neon/Essentials/Jupyter/data.txt\")\n",
    "\n",
    "# SQL like format to interact with DF\n",
    "result_df = df.selectExpr(\"explode(split(value, ' ')) as word\") \\\n",
    "              .groupBy(\"word\").count() \\\n",
    "              .orderBy(\"count\")  # .orderBy(desc(\"count\"))\n",
    "\n",
    "# Printing the resultant top 5 words\n",
    "print(f\"The resultant set was : {result_df.take(5)}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2cd01-d119-44ba-8784-feda05f9c6f3",
   "metadata": {},
   "source": [
    "### Sample Program - 10 : .CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f3aa1-e72f-4422-8e3a-dba61e87ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# head -10 ./products.csv\n",
    "\n",
    "\"\"\"\n",
    "    The program for play around with .csv files\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DF-Creation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./Dependencies/products.csv\", header=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# For the correct datatype detection\n",
    "df = spark.read.csv(\"./Dependencies/products.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Explicit schema ..\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# The schema we going to use ..\n",
    "schema = StructType([\n",
    "\tStructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
    "\tStructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "\tStructField(name=\"category\", dataType=StringType(), nullable=True),\n",
    "\tStructField(name=\"quantity\", dataType=IntegerType(), nullable=True),\n",
    "\tStructField(name=\"price\", dataType=DoubleType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using the schame to modify the default one\n",
    "df = spark.read.csv(\"./Dependencies/products.csv\", header=True, schema=schema)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85699b34-b7c2-409b-a1a2-a4ed21abf130",
   "metadata": {},
   "source": [
    "### Sample Program - 11 : .Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012a896-f64e-4813-8eb7-dd35aeb6b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The program for play around with .json files\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DF-Creation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Single line json file\n",
    "df = spark.read.json(\"./Dependencies/products_singleline.json\")\n",
    "df.printSchema()\n",
    "\n",
    "# Multi line json file\n",
    "df = spark.read.json(\"./Dependencies/products_multiline.json\", multiLine=True)\n",
    "df.printSchema()\n",
    "\n",
    "# Write the data into .parquet file\n",
    "df.write.parquet(\"./Dependencies/products_parquet.parquet\")\n",
    "\n",
    "# Read the data from .parquet file\n",
    "df_data = spark.read.parquet(\"./Dependencies/products_parquet.parquet\")\n",
    "\n",
    "df_data.printSchema()\n",
    "df_data.show(5)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99708d17-f542-4066-b260-8f677a3b5a48",
   "metadata": {},
   "source": [
    "### Sample Program - 12 : Few operations on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d2e5b-d865-4f08-8d56-0eb4365a2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Program which demonstrate few operations on DataFrames\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DF-Operations\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./Dependencies/stocks.txt\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Select specific column\n",
    "selected_column = df.select('id', 'name')\n",
    "selected_column.show(10)\n",
    "\n",
    "# Filter it respected to the given condition\n",
    "filtered_column = df.filter(df.quantity > 20)\n",
    "filtered_column.show(10)\n",
    "\n",
    "# GroupBy()\n",
    "grouped_column = df.groupBy(\"category\").agg({\"quantity\": \"sum\", \"price\": \"avg\"})\n",
    "grouped_column.show()\n",
    "\n",
    "# Join() using ID\n",
    "df2 = df.select(\"id\", \"category\").limit(10)\n",
    "joined_column = df.join(df2, \"id\", \"inner\")\n",
    "joined_column.show()\n",
    "\n",
    "# OrderBy sorting but single column\n",
    "sorted_column = df.orderBy(\"price\")\n",
    "sorted_column.show(10)\n",
    "\n",
    "# Sorting but multi column\n",
    "from pyspark.sql.functions import col, desc\n",
    "sorted_column_data = df.orderBy(col(\"price\").desc(), col(\"id\").desc())\n",
    "sorted_column_data.show(10)\n",
    "\n",
    "# Getting Unique rows\n",
    "unique_column = df.select(\"category\").distinct()\n",
    "unique_column.show()\n",
    "\n",
    "# Remove / deleting / dropping the columns\n",
    "retained_column = df.drop(\"quantity\", \"category\")\n",
    "retained_column.show(10)\n",
    "\n",
    "# Add a new column to the table\n",
    "data_with_new_column = df.withColumn(\"revenue\", df.quantity * df.price)\n",
    "data_with_new_column.show(10)\n",
    "\n",
    "# Renaming the column\n",
    "renamed_column = df.withColumnRenamed(\"price\", \"cost\")\n",
    "renamed_column.show(10)\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34089ab0-64b7-47b9-af2a-3b9d8b1d56fc",
   "metadata": {},
   "source": [
    "### Sample Program - 13 : Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44f935-0fbf-4c45-b135-48b7e9a37953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Using Spark SQL concept, using SQL queries by registering as TempView\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DF-SQL-Operation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./Dependencies/persons.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Register temporary table (It will be registered in the spark session, we can access it using spark session object)\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Run SQL queries over the created temp view table which is registered under spark session\n",
    "result = spark.sql(\"select * from my_table where age > 25\")\n",
    "result.show()\n",
    "\n",
    "# To check whether the table with this name exists or not under the mentioned spark session\n",
    "print(spark.catalog.tableExists(\"my_table\"))\n",
    "\n",
    "# Drop the temp view table\n",
    "spark.catalog.dropTempView(\"my_table\")\n",
    "print(spark.catalog.tableExists(\"my_table\"))\n",
    "\n",
    "# SUB QUERIES\n",
    "# -----------\n",
    "\n",
    "# Create new DataFrames\n",
    "employee_data = [\n",
    "    (1, \"John\"), (2, \"Alice\"), (3, \"Bob\"), (4, \"Emily\"),\n",
    "    (5, \"David\"), (6, \"Sarah\"), (7, \"Michael\"), (8, \"Lisa\"),\n",
    "    (9, \"William\")]\n",
    "\n",
    "employees = spark.createDataFrame(employee_data, [\"id\", \"name\"])\n",
    "\n",
    "salary_data = [\n",
    "    (\"HR\", 1, 60000), (\"HR\", 2, 55000), (\"HR\", 3, 58000),\n",
    "    (\"IT\", 4, 70000), (\"IT\", 5, 72000), (\"IT\", 6, 68000),\n",
    "    (\"Sales\", 7, 75000), (\"Sales\", 8, 78000), (\"Sales\", 9, 77000)]\n",
    "\n",
    "salaries = spark.createDataFrame(salary_data, [\"department\", \"id\", \"salary\"])\n",
    "\n",
    "# Register as temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "salaries.createOrReplaceTempView(\"salaries\")\n",
    "\n",
    "# Subquery to find employees with salaries above average\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name\n",
    "    FROM employees\n",
    "    WHERE id IN (\n",
    "        SELECT id\n",
    "        FROM salaries\n",
    "        WHERE salary > (SELECT AVG(salary) FROM salaries)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735a662-0e32-45dc-9ef3-9e14c9fbd7e6",
   "metadata": {},
   "source": [
    "### Sample Program - 14 : Windows Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53715c7b-fe64-4e21-8152-155e095ab59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Windows Functions\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DF-SQL-Operation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "employee_salary = spark.sql(\"\"\"\n",
    "    select  salaries.*, employees.name\n",
    "    from salaries \n",
    "    left join employees on salaries.id = employees.id\n",
    "\"\"\")\n",
    "\n",
    "employee_salary.show()\n",
    "\n",
    "# Create a window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "\n",
    "# Calculate the rank of employees within each department based on salary\n",
    "employee_salary.withColumn(\"rank\", F.rank().over(window_spec)).show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7b7c7-0d40-430a-a854-54990e9537ad",
   "metadata": {},
   "source": [
    "### Sample Program - 15 : BroadCast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b6503-d52f-4431-9bea-ccd6bbef4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    BroadCast\n",
    "\"\"\"\n",
    "\n",
    "# Importing neccessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Shared-Variables\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Creating a broadcast variable\n",
    "words_new = spark.sparkContext.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"])\n",
    "\n",
    "# Assinging the broadcast value to a new variable\n",
    "data = words_new.value \n",
    "\n",
    "# Fetching a specific value and printing\n",
    "print(f\"Printing a particular element in RDD = {words_new.value[2]}\")\n",
    "\n",
    "# Stoping the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feefdf5-a72e-413a-9c59-cc2a0763342d",
   "metadata": {},
   "source": [
    "### Sample Program - 16 : Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0292a-d301-4aca-82ef-ce2c50abeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Accumulator : aggregating the things using associative and cumulative properties / expression\n",
    "\"\"\"\n",
    "\n",
    "# Importing neccessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Shared-Variables\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Create a accumulator value named 'num'\n",
    "num = spark.sparkContext.accumulator(10) \n",
    "\n",
    "# Defining the function for each element execution\n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "\n",
    "# Creating the RDD using parallelize\n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "\n",
    "# Using foreach to loop and iterate each element\n",
    "rdd.foreach(f) \n",
    "\n",
    "# Accessing the num (shared variable)\n",
    "final = num.value \n",
    "\n",
    "# Printing the final result\n",
    "print(f\"Accumulated value is : {final}\")\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b11f2-0b65-4860-b37a-de068361a19f",
   "metadata": {},
   "source": [
    "### Sample Program - 17 : Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cec8110-1727-4ed3-8925-c367f920d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+------------------------------+-------+--------------------+-------------+------+\n",
      "|id |first_name|last_name|fav_movie                     |salary |image_url           |date_of_birth|active|\n",
      "+---+----------+---------+------------------------------+-------+--------------------+-------------+------+\n",
      "|102|Kenny     |Bobien   |[Men in Black II, Home Alone] |4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "|103|Sara      |Devine   |[Men in Black I, Home Alone]  |4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "|101|Robert    |Ownes    |[Men in Black III, Home Alone]|4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "+---+----------+---------+------------------------------+-------+--------------------+-------------+------+\n",
      "\n",
      "+---+----------+---------+------------------------------+-------+--------------------+-------------+------+\n",
      "|id |first_name|last_name|fav_movie                     |salary |image_url           |date_of_birth|active|\n",
      "+---+----------+---------+------------------------------+-------+--------------------+-------------+------+\n",
      "|102|Kenny     |Bobien   |[Men in Black II, Home Alone] |4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "|103|Sara      |Devine   |[Men in Black I, Home Alone]  |4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "|101|Robert    |Ownes    |[Men in Black III, Home Alone]|4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "|102|Kenny     |Bobien   |[Men in Black II, Home Alone] |4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "|103|Sara      |Devine   |[Men in Black I, Home Alone]  |4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "|101|Robert    |Ownes    |[Men in Black III, Home Alone]|4300.64|http://someimage.com|1964-08-18   |true  |\n",
      "+---+----------+---------+------------------------------+-------+--------------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Union\n",
    "\"\"\"\n",
    "\n",
    "# Importing neccessary packages\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Union-Rows\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "person_row = Row(101, \"Robert\", \"Ownes\", [\"Men in Black III\", \"Home Alone\"] ,4300.64, \"http://someimage.com\", \"1964-08-18\", True)\n",
    "person_rows_list = [Row(102, \"Kenny\", \"Bobien\", [\"Men in Black II\", \"Home Alone\"] ,4300.64, \"http://someimage.com\", \"1964-08-18\", True),\n",
    "                     Row(103, \"Sara\", \"Devine\", [\"Men in Black I\", \"Home Alone\"] ,4300.64, \"http://someimage.com\", \"1964-08-18\", True)]\n",
    "\n",
    "person_rows_list.append(person_row)\n",
    "\n",
    "headers = ['id', 'first_name', 'last_name', 'fav_movie', 'salary', 'image_url', 'date_of_birth', 'active']\n",
    "\n",
    "new_person_df = spark.createDataFrame(person_rows_list, headers)\n",
    "\n",
    "new_person_df.show(truncate=False)\n",
    "\n",
    "# Using Union : we can combine the new DF with existing DF\n",
    "union_df = new_person_df.union(new_person_df)\n",
    "\n",
    "union_df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834741f-dbe3-4dc7-8aaa-737273dda476",
   "metadata": {},
   "source": [
    "### Sample Program - 18 : User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32581fa9-241e-4dcc-a926-f7eed5e20a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+\n",
      "|  Name|Mark|grade|\n",
      "+------+----+-----+\n",
      "| swami|  85|    B|\n",
      "|nathan|  95|    A|\n",
      "| rahul|  55|    F|\n",
      "+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Spark User Defined Functions were poorly optimized, we need to ignore as much as possible.\n",
    "\"\"\"\n",
    "\n",
    "# Importing neccessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"User-Func\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [('swami', 85), ('nathan', 95), ('rahul', 55)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=['Name', 'Mark'])\n",
    "\n",
    "def gradder(mark: int) -> str:\n",
    "    \n",
    "    grade = ''\n",
    "    if mark > 100:\n",
    "        return 'Cheating'\n",
    "    elif mark > 90:\n",
    "        return 'A'\n",
    "    elif mark > 80:\n",
    "        return 'B'\n",
    "    elif mark > 70:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'F'\n",
    "\n",
    "gradderUDF = udf(gradder)\n",
    "\n",
    "df_grades = df.select(\"Name\", \"Mark\", gradderUDF(col(\"Mark\")).alias(\"grade\"))\n",
    "\n",
    "df_grades.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9e6e8-d5c3-4322-9d74-6737b0f0412e",
   "metadata": {},
   "source": [
    "### Sample Program - 19 : Renaming a column in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7eecaae-a9a2-4dc2-8b16-f88baeff7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin_code: string (nullable = true)\n",
      " |-- origin_airport: string (nullable = true)\n",
      " |-- origin_city: string (nullable = true)\n",
      " |-- origin_state: string (nullable = true)\n",
      " |-- dest_code: string (nullable = true)\n",
      " |-- dest_airport: string (nullable = true)\n",
      " |-- dest_city: string (nullable = true)\n",
      " |-- dest_state: string (nullable = true)\n",
      " |-- flight_count: integer (nullable = true)\n",
      "\n",
      "+-----------+--------------------+------------+------------+---------+--------------------+---------+----------+------------+\n",
      "|origin_code|      origin_airport| origin_city|origin_state|dest_code|        dest_airport|dest_city|dest_state|flight_count|\n",
      "+-----------+--------------------+------------+------------+---------+--------------------+---------+----------+------------+\n",
      "|        BQN|Rafael HernÃ¡ndez ...|   Aguadilla|          PR|      MCO|Orlando Internati...|  Orlando|        FL|         441|\n",
      "|        PHL|Philadelphia Inte...|Philadelphia|          PA|      MCO|Orlando Internati...|  Orlando|        FL|        4869|\n",
      "|        MCI|Kansas City Inter...| Kansas City|          MO|      IAH|George Bush Inter...|  Houston|        TX|        1698|\n",
      "|        SPI|Abraham Lincoln C...| Springfield|          IL|      ORD|Chicago O'Hare In...|  Chicago|        IL|         998|\n",
      "|        SNA|John Wayne Airpor...|   Santa Ana|          CA|      PHX|Phoenix Sky Harbo...|  Phoenix|        AZ|        3846|\n",
      "+-----------+--------------------+------------+------------+---------+--------------------+---------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+----------------------------+\n",
      "|count(dest_airport)|count(DISTINCT dest_airport)|\n",
      "+-------------------+----------------------------+\n",
      "|               4693|                         322|\n",
      "+-------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Renaming a column in DataFrame\n",
    "\"\"\"\n",
    "\n",
    "# Importing neccessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Aggregation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "flight_file = './Dependencies/flight-summary.csv'\n",
    "flight_summary_df = spark.read.csv(flight_file, header=True, inferSchema=True)\n",
    "\n",
    "flight_summary_df = flight_summary_df.withColumnRenamed('count', 'flight_count')\n",
    "\n",
    "flight_summary_df.printSchema()\n",
    "flight_summary_df.show(5)\n",
    "\n",
    "flight_summary_df.select(count(\"dest_airport\"), countDistinct(\"dest_airport\")).show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816c044-8ae9-4de5-94d2-3d32f3612c0b",
   "metadata": {},
   "source": [
    "### Sample Program - 20 : Catch and Persists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8c84c-f471-483c-aacc-dc984ac3c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to cache data in memory \n",
    "df_from_csv.cache()\n",
    "\n",
    "# How to persist data in local disk \n",
    "df_from_csv.persist(storageLevel=StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd112c4-96d9-44de-82da-b1b013df7a09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'org'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01morg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapache\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSaveMode\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'org'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ebb591-6dd7-4622-9a28-de71cfdc5fb0",
   "metadata": {},
   "source": [
    "## Test Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14847de-a274-4b3b-ad20-4609b05edccb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Program - 01 \n",
    "\n",
    "#### Challenge : Daily Temperature (F to C conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf61fd4d-da0a-4185-97e9-7233f4ae0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Day1 = 15.00 C\n",
      "In Day2 = 14.00 C\n",
      "In Day3 = 12.00 C\n",
      "In Day4 = 13.00 C\n",
      "In Day5 = 11.00 C\n",
      "In Day6 = 12.00 C\n",
      "In Day7 = 13.00 C\n",
      "[15.0, 14.000000000000002, 12.000000000000002, 13.0, 10.999999999999998, 12.000000000000002, 13.0]\n",
      "Around 4 days have Temperature greater then 13 C\n",
      "They were : [('Day1', 15.0), ('Day2', 14.000000000000002), ('Day4', 13.0), ('Day7', 13.0)]\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Daily-Temp\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Python list as a data\n",
    "fahrenheit = [('Day1', 59), ('Day2',57.2), ('Day3',53.6), ('Day4',55.4), ('Day5',51.8), ('Day6',53.6), ('Day7',55.4)]\n",
    "\n",
    "# Creating an RDD using parallelize()\n",
    "rdd = spark.sparkContext.parallelize(fahrenheit)\n",
    "\n",
    "# Converting the Fahrenheit to Celsius\n",
    "mapped_rdd = rdd.map(lambda x: (x[0], (x[1] - 32) * (5 / 9)))\n",
    "\n",
    "# This is list, collect() gives list not an RDD\n",
    "rdd_result = mapped_rdd.collect()\n",
    "\n",
    "# Print the List\n",
    "for day, value in rdd_result:\n",
    "    print(f\"In {day} = {value:.2f} C\")\n",
    "\n",
    "# Create new RDD which has only numbers\n",
    "new_rdd = mapped_rdd.map(lambda x: x[1])\n",
    "\n",
    "print(new_rdd.collect())\n",
    "\n",
    "# Fetch the value equal or greater than 13\n",
    "reduced_rdd = mapped_rdd.filter(lambda x: x[1] >= 13)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Around {reduced_rdd.count()} days have Temperature greater then 13 C\\nThey were : {reduced_rdd.collect()}\")\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9007f1-d3ec-465b-a183-55fedfd9bf01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Program - 02\n",
    "\n",
    "#### Challenge : Preprocessing and Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5401be39-2626-4a5c-ad9f-6ebb6547ef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|Product                   |\n",
      "+--------------------------+\n",
      "|Wired Headphones          |\n",
      "|Macbook Pro Laptop        |\n",
      "|Apple Airpods Headphones  |\n",
      "|iPhone                    |\n",
      "|Lightning Charging Cable  |\n",
      "|Bose SoundSport Headphones|\n",
      "|USB-C Charging Cable      |\n",
      "|AAA Batteries (4-pack)    |\n",
      "|20in Monitor              |\n",
      "|27in FHD Monitor          |\n",
      "+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|Order ID|Product|Quantity Ordered|Price Each|Order Date|Purchase Address|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|    NULL|   NULL|            NULL|      NULL|      NULL|            NULL|\n",
      "|    NULL|   NULL|            NULL|      NULL|      NULL|            NULL|\n",
      "|    NULL|Product|            NULL|      NULL|Order Date|Purchase Address|\n",
      "|    NULL|   NULL|            NULL|      NULL|      NULL|            NULL|\n",
      "|    NULL|Product|            NULL|      NULL|Order Date|Purchase Address|\n",
      "|    NULL|Product|            NULL|      NULL|Order Date|Purchase Address|\n",
      "|    NULL|   NULL|            NULL|      NULL|      NULL|            NULL|\n",
      "|    NULL|   NULL|            NULL|      NULL|      NULL|            NULL|\n",
      "|    NULL|   NULL|            NULL|      NULL|      NULL|            NULL|\n",
      "|    NULL|   NULL|            NULL|      NULL|      NULL|            NULL|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|Order ID|Product|Quantity Ordered|Price Each|Order Date|Purchase Address|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|             18289|       18289|              18289|             18289|         18289|               18289|\n",
      "|   mean|185328.81672043304|        NULL| 1.1246104215648751|184.43102630000277|          NULL|                NULL|\n",
      "| stddev| 5061.520829296985|        NULL|0.43640973695741925| 330.9133771769665|          NULL|                NULL|\n",
      "|    min|            176558|20in Monitor|                  1|              2.99|04/01/19 03:09|1 14th St, New Yo...|\n",
      "|    max|            194094|      iPhone|                  7|            1700.0|05/01/19 04:25|999 Pine St, Bost...|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n",
      "+--------------------------------------+---------------------------------+\n",
      "|Purchase Address                      |split(Purchase Address, ,, -1)[2]|\n",
      "+--------------------------------------+---------------------------------+\n",
      "|917 1st St, Dallas, TX 75001          | TX 75001                        |\n",
      "|682 Chestnut St, Boston, MA 02215     | MA 02215                        |\n",
      "|669 Spruce St, Los Angeles, CA 90001  | CA 90001                        |\n",
      "|669 Spruce St, Los Angeles, CA 90001  | CA 90001                        |\n",
      "|333 8th St, Los Angeles, CA 90001     | CA 90001                        |\n",
      "|381 Wilson St, San Francisco, CA 94016| CA 94016                        |\n",
      "|668 Center St, Seattle, WA 98101      | WA 98101                        |\n",
      "|790 Ridge St, Atlanta, GA 30301       | GA 30301                        |\n",
      "|915 Willow St, San Francisco, CA 94016| CA 94016                        |\n",
      "|83 7th St, Boston, MA 02215           | MA 02215                        |\n",
      "+--------------------------------------+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------+-----+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|          City|State|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------+-----+\n",
      "|  176558|USB-C Charging Cable|               2|     11.95|04/19/19 08:46|917 1st St, Dalla...|        Dallas|   TX|\n",
      "|  176559|Bose SoundSport H...|               1|     99.99|04/07/19 22:30|682 Chestnut St, ...|        Boston|   MA|\n",
      "|  176560|        Google Phone|               1|     600.0|04/12/19 14:38|669 Spruce St, Lo...|   Los Angeles|   CA|\n",
      "|  176560|    Wired Headphones|               1|     11.99|04/12/19 14:38|669 Spruce St, Lo...|   Los Angeles|   CA|\n",
      "|  176561|    Wired Headphones|               1|     11.99|04/30/19 09:27|333 8th St, Los A...|   Los Angeles|   CA|\n",
      "|  176562|USB-C Charging Cable|               1|     11.95|04/29/19 13:03|381 Wilson St, Sa...| San Francisco|   CA|\n",
      "|  176563|Bose SoundSport H...|               1|     99.99|04/02/19 07:46|668 Center St, Se...|       Seattle|   WA|\n",
      "|  176564|USB-C Charging Cable|               1|     11.95|04/12/19 10:58|790 Ridge St, Atl...|       Atlanta|   GA|\n",
      "|  176565|  Macbook Pro Laptop|               1|    1700.0|04/24/19 10:38|915 Willow St, Sa...| San Francisco|   CA|\n",
      "|  176566|    Wired Headphones|               1|     11.99|04/08/19 14:05|83 7th St, Boston...|        Boston|   MA|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+------+-------------------+----------+-----+\n",
      "|             Product|    Order Date|        StoreAddress|          City|State|OrderID|Quantity| Price|          OrderDate|ReportYear|Month|\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+------+-------------------+----------+-----+\n",
      "|USB-C Charging Cable|04/19/19 08:46|917 1st St, Dalla...|        Dallas|   TX| 176558|       2| 11.95|2019-04-19 08:46:00|      2019|    4|\n",
      "|Bose SoundSport H...|04/07/19 22:30|682 Chestnut St, ...|        Boston|   MA| 176559|       1| 99.99|2019-04-07 22:30:00|      2019|    4|\n",
      "|        Google Phone|04/12/19 14:38|669 Spruce St, Lo...|   Los Angeles|   CA| 176560|       1| 600.0|2019-04-12 14:38:00|      2019|    4|\n",
      "|    Wired Headphones|04/12/19 14:38|669 Spruce St, Lo...|   Los Angeles|   CA| 176560|       1| 11.99|2019-04-12 14:38:00|      2019|    4|\n",
      "|    Wired Headphones|04/30/19 09:27|333 8th St, Los A...|   Los Angeles|   CA| 176561|       1| 11.99|2019-04-30 09:27:00|      2019|    4|\n",
      "|USB-C Charging Cable|04/29/19 13:03|381 Wilson St, Sa...| San Francisco|   CA| 176562|       1| 11.95|2019-04-29 13:03:00|      2019|    4|\n",
      "|Bose SoundSport H...|04/02/19 07:46|668 Center St, Se...|       Seattle|   WA| 176563|       1| 99.99|2019-04-02 07:46:00|      2019|    4|\n",
      "|USB-C Charging Cable|04/12/19 10:58|790 Ridge St, Atl...|       Atlanta|   GA| 176564|       1| 11.95|2019-04-12 10:58:00|      2019|    4|\n",
      "|  Macbook Pro Laptop|04/24/19 10:38|915 Willow St, Sa...| San Francisco|   CA| 176565|       1|1700.0|2019-04-24 10:38:00|      2019|    4|\n",
      "|    Wired Headphones|04/08/19 14:05|83 7th St, Boston...|        Boston|   MA| 176566|       1| 11.99|2019-04-08 14:05:00|      2019|    4|\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+------+-------------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, to_timestamp, year, month\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Data-Preprocessing-Cleansing\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Creating the DF throught the external file\n",
    "df = spark.read.csv(\"./Dependencies/Sales_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Checking whether data loaded correctly or not\n",
    "df.select(\"Product\").distinct().show(10, truncate=False)\n",
    "\n",
    "# Checking whether Null value exists or not \n",
    "df.filter(col(\"Order ID\").isNull() == True).show(10)\n",
    "\n",
    "# To remove Null values / Bad data value removed\n",
    "not_null_df = df.na.drop(\"any\")\n",
    "\n",
    "# Checking whether Null value exists or not \n",
    "not_null_df.filter(col(\"Product\").isNull() == True).show(10)\n",
    "\n",
    "# Checking basic info of all columns\n",
    "not_null_df.describe(\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\").show(10)\n",
    "\n",
    "# Checking the Purchase Address\n",
    "not_null_df.select(\"Purchase Address\", split(col(\"Purchase Address\"), \",\").getItem(2)).show(10, truncate=False)\n",
    "\n",
    "not_null_df = not_null_df.withColumn(\"City\", split(col(\"Purchase Address\"), \",\").getItem(1)) \\\n",
    "                         .withColumn(\"State\", split(split(col(\"Purchase Address\"), \",\").getItem(2), ' ').getItem(1))\n",
    "\n",
    "not_null_df.show(10)\n",
    "\n",
    "not_null_df = (not_null_df.withColumn(\"OrderID\", col(\"Order ID\").cast(IntegerType()))\n",
    "                              .withColumn (\"Quantity\", col(\"Quantity Ordered\").cast(IntegerType()))\n",
    "                              .withColumn (\"Price\", col(\"Price Each\").cast(FloatType()))\n",
    "                              .withColumn (\"OrderDate\", to_timestamp(col(\"Order Date\"), \"MM/dd/yy HH:mm\"))\n",
    "                              .withColumnRenamed(\"Purchase Address\", \"StoreAddress\")\n",
    "                              .drop(\"Order ID\")\n",
    "                              .drop(\"Quantity Ordered\")\n",
    "                              .drop (\"Price Each\")\n",
    "                              .drop (\"Purchase Address\"))\n",
    "\n",
    "not_null_df = (not_null_df.withColumn (\"ReportYear\", year(col (\"OrderDate\")))\n",
    "                              .withColumn (\"Month\", month(col(\"OrderDate\"))))\n",
    "\n",
    "not_null_df.show(10, truncate=True)\n",
    "\n",
    "output_file = \"./Dependencies/Sales_output_data\"\n",
    "\n",
    "not_null_df.write.mode(\"overwrite\").partitionBy(\"ReportYear\", \"Month\").parquet(output_file)\n",
    "\n",
    "# Stopping the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa108e5e-47e1-481d-b0d2-b910df3cc84d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Program - 03 \n",
    "\n",
    "#### Challenge : Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cd92bdb7-3f00-487d-ba4b-9fca33e5762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+------+-------------------+-----+\n",
      "|             Product|    Order Date|        StoreAddress|          City|State|OrderID|Quantity| Price|          OrderDate|Month|\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+------+-------------------+-----+\n",
      "|USB-C Charging Cable|04/19/19 08:46|917 1st St, Dalla...|        Dallas|   TX| 176558|       2| 11.95|2019-04-19 08:46:00|    4|\n",
      "|Bose SoundSport H...|04/07/19 22:30|682 Chestnut St, ...|        Boston|   MA| 176559|       1| 99.99|2019-04-07 22:30:00|    4|\n",
      "|        Google Phone|04/12/19 14:38|669 Spruce St, Lo...|   Los Angeles|   CA| 176560|       1| 600.0|2019-04-12 14:38:00|    4|\n",
      "|    Wired Headphones|04/12/19 14:38|669 Spruce St, Lo...|   Los Angeles|   CA| 176560|       1| 11.99|2019-04-12 14:38:00|    4|\n",
      "|    Wired Headphones|04/30/19 09:27|333 8th St, Los A...|   Los Angeles|   CA| 176561|       1| 11.99|2019-04-30 09:27:00|    4|\n",
      "|USB-C Charging Cable|04/29/19 13:03|381 Wilson St, Sa...| San Francisco|   CA| 176562|       1| 11.95|2019-04-29 13:03:00|    4|\n",
      "|Bose SoundSport H...|04/02/19 07:46|668 Center St, Se...|       Seattle|   WA| 176563|       1| 99.99|2019-04-02 07:46:00|    4|\n",
      "|USB-C Charging Cable|04/12/19 10:58|790 Ridge St, Atl...|       Atlanta|   GA| 176564|       1| 11.95|2019-04-12 10:58:00|    4|\n",
      "|  Macbook Pro Laptop|04/24/19 10:38|915 Willow St, Sa...| San Francisco|   CA| 176565|       1|1700.0|2019-04-24 10:38:00|    4|\n",
      "|    Wired Headphones|04/08/19 14:05|83 7th St, Boston...|        Boston|   MA| 176566|       1| 11.99|2019-04-08 14:05:00|    4|\n",
      "|        Google Phone|04/18/19 17:18|444 7th St, Los A...|   Los Angeles|   CA| 176567|       1| 600.0|2019-04-18 17:18:00|    4|\n",
      "|Lightning Chargin...|04/15/19 12:18|438 Elm St, Seatt...|       Seattle|   WA| 176568|       1| 14.95|2019-04-15 12:18:00|    4|\n",
      "|27in 4K Gaming Mo...|04/16/19 19:23|657 Hill St, Dall...|        Dallas|   TX| 176569|       1|389.99|2019-04-16 19:23:00|    4|\n",
      "|AA Batteries (4-p...|04/22/19 15:09|186 12th St, Dall...|        Dallas|   TX| 176570|       1|  3.84|2019-04-22 15:09:00|    4|\n",
      "|Lightning Chargin...|04/19/19 14:29|253 Johnson St, A...|       Atlanta|   GA| 176571|       1| 14.95|2019-04-19 14:29:00|    4|\n",
      "|Apple Airpods Hea...|04/04/19 20:30|149 Dogwood St, N...| New York City|   NY| 176572|       1| 150.0|2019-04-04 20:30:00|    4|\n",
      "|USB-C Charging Cable|04/27/19 18:41|214 Chestnut St, ...| San Francisco|   CA| 176573|       1| 11.95|2019-04-27 18:41:00|    4|\n",
      "|        Google Phone|04/03/19 19:42|20 Hill St, Los A...|   Los Angeles|   CA| 176574|       1| 600.0|2019-04-03 19:42:00|    4|\n",
      "|USB-C Charging Cable|04/03/19 19:42|20 Hill St, Los A...|   Los Angeles|   CA| 176574|       1| 11.95|2019-04-03 19:42:00|    4|\n",
      "|AAA Batteries (4-...|04/27/19 00:30|433 Hill St, New ...| New York City|   NY| 176575|       1|  2.99|2019-04-27 00:30:00|    4|\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+---------+\n",
      "|Month|TotalSale|\n",
      "+-----+---------+\n",
      "|    4|3385499.8|\n",
      "+-----+---------+\n",
      "\n",
      "+--------------+----+\n",
      "|          City|Quan|\n",
      "+--------------+----+\n",
      "| San Francisco|4987|\n",
      "+--------------+----+\n",
      "\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+-----+-------------------+-----+\n",
      "|             Product|    Order Date|        StoreAddress|          City|State|OrderID|Quantity|Price|          OrderDate|Month|\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+-----+-------------------+-----+\n",
      "|AAA Batteries (4-...|04/16/19 00:04|713 Sunset St, Ne...| New York City|   NY| 177017|       5| 2.99|2019-04-16 00:04:00|    4|\n",
      "+--------------------+--------------+--------------------+--------------+-----+-------+--------+-----+-------------------+-----+\n",
      "\n",
      "+-------+-----+--------------------------+\n",
      "|OrderID|State|ProductList               |\n",
      "+-------+-----+--------------------------+\n",
      "|176572 |NY   |[Apple Airpods Headphones]|\n",
      "|176575 |NY   |[AAA Batteries (4-pack)]  |\n",
      "|176579 |NY   |[AA Batteries (4-pack)]   |\n",
      "|176590 |NY   |[Google Phone]            |\n",
      "|176599 |NY   |[Lightning Charging Cable]|\n",
      "|176613 |NY   |[USB-C Charging Cable]    |\n",
      "|176637 |NY   |[AAA Batteries (4-pack)]  |\n",
      "|176652 |NY   |[LG Washing Machine]      |\n",
      "|176656 |NY   |[Google Phone]            |\n",
      "|176671 |NY   |[AAA Batteries (4-pack)]  |\n",
      "+-------+-----+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, round, sum, collect_list\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SalesAnalytics\").getOrCreate()\n",
    "\n",
    "source_output_data = \"./Dependencies/Sales_output_data/\"\n",
    "partition = \"ReportYear=2019\"\n",
    "\n",
    "sales2019_df = spark.read.parquet(f\"{source_output_data}{partition}\")\n",
    "sales2019_df.show(truncate=True)\n",
    "\n",
    "new_sales2019_df = sales2019_df.select(\"OrderID\", \"Month\", \"Price\", \"Quantity\", expr(\"Price * Quantity\").alias(\"Sales\"))\n",
    "\n",
    "new_one = new_sales2019_df.groupBy(\"Month\").agg(round(sum(\"Sales\"),2).alias(\"TotalSale\")) \\\n",
    "                .orderBy(\"TotalSale\", ascending=False).limit(1).show()\n",
    "\n",
    "sales2019_df.groupBy(\"City\") \\\n",
    "            .agg(sum(\"Quantity\").alias(\"Quan\")) \\\n",
    "            .orderBy(\"Quan\", ascending=False) \\\n",
    "            .limit(1).show()\n",
    "\n",
    "sales2019_df.where(col(\"State\") == \"NY\").orderBy(\"Quantity\", ascending=False).limit(1).show()\n",
    "\n",
    "sales_q4_df = (sales2019_df.where(col(\"State\") == 'NY')\n",
    ".orderBy (\"OrderID\", \"Product\" )\n",
    ".groupBy (\"OrderID\" ,\"State\")\n",
    ".agg(collect_list(\"Product\").alias(\"ProductList\")))\n",
    "\n",
    "sales_q4_df.show(10, truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
